{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ECMM426/ECMM441 - Convolutional Neural Networks and Transfer Learning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_5Tkl9SzhjN"
   },
   "source": [
    "<H2 style=\"text-align: center\">Convolutional Neural Networks and Transfer Learning</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8f95r3cyd1S"
   },
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "In this tutorial, we are going to use PyTorch, the cutting-edge deep learning framework to complete our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT-4QzaoAxQX"
   },
   "source": [
    "### STL-10 Dataset\n",
    "\n",
    "Details on STL-10 dataset can be found here: https://cs.stanford.edu/~acoates/stl10"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C7KkXL5OA8WX"
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "## Create dataloader, in PyTorch, we feed the trainer data with use of dataloader\n",
    "## We create dataloader with dataset from torchvision, \n",
    "## and we dont have to download it seperately, all automatically done\n",
    "\n",
    "# Define batch size, batch size is how much data you feed for training in one iteration\n",
    "batch_size_train = 256 # We use a small batch size here for training\n",
    "batch_size_test = 1024 #\n",
    "# define how image transformed\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "#image datasets\n",
    "train_dataset = torchvision.datasets.STL10('dataset/', \n",
    "                                           split='train', \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "test_dataset = torchvision.datasets.STL10('dataset/', \n",
    "                                          split='test', \n",
    "                                          download=True,\n",
    "                                          transform=image_transform)\n",
    "#data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size_test, \n",
    "                                          shuffle=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdYlqJ7xDvJa"
   },
   "source": [
    "###Example Image\n",
    "Since in the data loader, we already have transformed images, we need to apply inverse transformation on the image to see the original image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U4FrTxdxDwnE"
   },
   "source": [
    "# import library\n",
    "import matplotlib.pyplot as plt\n",
    "#inverse normalization\n",
    "inv_normalize = torchvision.transforms.Normalize(mean=[-1.0, -1.0, -1.0], std=[1/0.5, 1/0.5, 1/0.5])\n",
    "# We can check the dataloader\n",
    "_, (example_datas, labels) = next(enumerate(test_loader))\n",
    "print(example_datas.shape)\n",
    "sample = example_datas[0]\n",
    "# show the data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(sample.permute(1, 2, 0))\n",
    "ax2.imshow(inv_normalize(sample).permute(1, 2, 0))\n",
    "print(\"Label: \" + str(labels[0]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUWGhlsMCYZD"
   },
   "source": [
    "### CNN Model\n",
    "You have to define trainable layers and put them inside a model. Have a look on the documentation of [PyTorch](https://pytorch.org/docs/stable/index.html) and read more about different layers and functionalities of PyTorch there. Here we are going to implement various versions of AlexNet model and use it for classification."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LMtPp5OeCakG"
   },
   "source": [
    "## Now we can start to build our CNN model\n",
    "## We first import the pytorch nn module and optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "## Then define the model class\n",
    "class AlexNet1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlexNet1, self).__init__()\n",
    "        # input channel 3, output channel 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        # relu non-linearity\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # max pooling\n",
    "        self.max_pool2d1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        # input channel 64, output channel 192\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2d2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        # input channel 192, output channel 384\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # input channel 384, output channel 256\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        # input channel 256, output channel 256\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.max_pool2d5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        # adaptive pooling\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "        #dropout layer\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        # linear layer\n",
    "        self.linear1 = nn.Linear(in_features=9216, out_features=4096, bias=True)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.linear2 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.max_pool2d1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool2d2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.max_pool2d5(x)\n",
    "        x = self.adapt_pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "class AlexNet2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlexNet2, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "            )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=9216, out_features=4096, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # B x C x H x W -> B x C*H*W\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class AlexNet3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlexNet3, self).__init__()\n",
    "        from torchvision import models\n",
    "        alexnet = models.alexnet(pretrained=False)\n",
    "        self.features = alexnet.features\n",
    "        self.avgpool = alexnet.avgpool\n",
    "        self.classifier = alexnet.classifier\n",
    "        self.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        # B x C x H x W -> B x C*H*W\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class AlexNet4(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlexNet4, self).__init__()\n",
    "        from torchvision import models\n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "        self.features = alexnet.features\n",
    "        self.avgpool = alexnet.avgpool\n",
    "        self.classifier = alexnet.classifier\n",
    "        self.classifier[6] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        # B x C x H x W -> B x C*H*W\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nayicPkJCkWy"
   },
   "source": [
    "### Model and Optimizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TjyEGZSdCk_i"
   },
   "source": [
    "## create model and optimizer\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.0005\n",
    "# define the model \n",
    "model = AlexNet2(10)\n",
    "# device: cuda or cpu\n",
    "device = \"cuda\"\n",
    "# map to device\n",
    "model = model.to(device)\n",
    "# make the parameters trainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oap6-7RKG0s7"
   },
   "source": [
    "### Meter\n",
    "Meter for keeping losses and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JeLH7fbOHDhH"
   },
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSk4VfO_C4tL"
   },
   "source": [
    "### Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NJeHMF_BC7bg"
   },
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "##define train function\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10000):\n",
    "    # meter\n",
    "    loss = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    for batch_idx, (data, target) in enumerate(tk0):\n",
    "        data, target = data.to(device), target.to(device)  # data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_this = F.cross_entropy(output, target)\n",
    "        loss_this.backward()\n",
    "        optimizer.step()\n",
    "        loss.update(loss_this.item(), target.shape[0])\n",
    "    print('Train: Average loss: {:.4f}\\n'.format(loss.avg))\n",
    "        \n",
    "##define test function\n",
    "def test(model, device, test_loader):\n",
    "    # meters\n",
    "    loss = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    # switch to test mode\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)  # data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss_this = F.cross_entropy(output, target) # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct_this = pred.eq(target.view_as(pred)).sum().item()\n",
    "            correct += correct_this\n",
    "            acc_this = correct_this/target.shape[0]*100.0\n",
    "            acc.update(acc_this, target.shape[0])\n",
    "            loss.update(loss_this.item(), target.shape[0])\n",
    "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        loss.avg, correct, len(test_loader.dataset), acc.avg))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzxivyrXDB7a"
   },
   "source": [
    "### Training Loop\n",
    "Training loop containing alternating train and test phase"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tna_R8TSDD4D"
   },
   "source": [
    "num_epoch = 20\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B19bDzBDIV4"
   },
   "source": [
    "### Summary\n",
    "Show the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LyQI2S9f0LVh"
   },
   "source": [
    "print(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9eztTJ4VDKCA"
   },
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 96, 96))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Skq4kzyt7PpN"
   },
   "source": [
    "##Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxLFKtvlV9jN"
   },
   "source": [
    "### TU-Berlin Dataset\n",
    "Details on TU-Berlin dataset can be found here: http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bxXqCPXvVI9J"
   },
   "source": [
    "import os\n",
    "if not os.path.exists('sketches_png.zip'):\n",
    "    !wget http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/sketches_png.zip\n",
    "    !unzip -q sketches_png.zip\n",
    "    !rm sketches_png.zip\n",
    "    !mv png tu_berlin"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kopBHyQu5K8t"
   },
   "source": [
    "### Split into Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bgkRoHjIub9M"
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "with open('tu_berlin/filelist.txt', 'r') as fp:\n",
    "    files = fp.read().splitlines()\n",
    "classes_str = [file.split('/')[0] for file in files]\n",
    "classes = np.unique(classes_str, return_inverse=True)[1]\n",
    "train_files, test_files, train_classes, test_classes = train_test_split(files, classes, train_size=0.3, test_size=0.1, stratify=classes)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_zDUF0R4xgR"
   },
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fCcARypv3hpL"
   },
   "source": [
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "class TUBerlin(data.Dataset):\n",
    "    def __init__(self, root, files, classes, transform=None):\n",
    "        self.root = root\n",
    "        self.files = files\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(os.path.join(self.root, self.files[item])).convert(mode='RGB')\n",
    "        class_ = self.classes[item]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, class_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEFyh0BDo-l4"
   },
   "source": [
    "###Intialization of the Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OTP0pmYG5oM9"
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# Define batch size, batch size is how much data you feed for training in one iteration\n",
    "batch_size_train = 256 # We use a small batch size here for training\n",
    "batch_size_test = 1024 #\n",
    "\n",
    "# define how image transformed\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.Resize((224, 224)),\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "#image datasets\n",
    "train_dataset = TUBerlin('tu_berlin/', train_files, train_classes, \n",
    "                         transform=image_transform)\n",
    "test_dataset = TUBerlin('tu_berlin/', test_files, test_classes, \n",
    "                        transform=image_transform)\n",
    "#data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size_test, \n",
    "                                          shuffle=True, num_workers=4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFhh272O7jkj"
   },
   "source": [
    "###Example Image\n",
    "Since in the data loader, we already have transformed images, we need to apply inverse transformation on the image to see the original image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4BSAiPFv7jkj"
   },
   "source": [
    "# import library\n",
    "import matplotlib.pyplot as plt\n",
    "#inverse normalization\n",
    "inv_normalize = torchvision.transforms.Normalize(mean=[-1.0, -1.0, -1.0], std=[1/0.5, 1/0.5, 1/0.5])\n",
    "# We can check the dataloader\n",
    "_, (example_datas, labels) = next(enumerate(train_loader))\n",
    "print(example_datas.shape)\n",
    "sample = example_datas[0]\n",
    "# show the data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(sample.permute(1, 2, 0))\n",
    "ax2.imshow(inv_normalize(sample).permute(1, 2, 0))\n",
    "print(\"Label: \" + str(labels[0]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__Wx5zUY7L92"
   },
   "source": [
    "### Model and Optimizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RxItpRN-7L93"
   },
   "source": [
    "## create model and optimizer\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.0005\n",
    "# define the model \n",
    "model = AlexNet4(250)\n",
    "# device: cuda or cpu\n",
    "device = \"cuda\"\n",
    "# map to device\n",
    "model = model.to(device)\n",
    "# make the parameters trainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_ET0FAw7Va_"
   },
   "source": [
    "### Training Loop\n",
    "Training loop containing alternating train and test phase"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4OyndchC7VbA"
   },
   "source": [
    "num_epoch = 20\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dxNHMSBPXNLp"
   },
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 224, 224))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}