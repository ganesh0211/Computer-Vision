{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ECMM426/ECMM441 - Object Detection on PASCAL VOC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNBK2Lo3WUc9"
   },
   "source": [
    "<H2 style=\"text-align: center\">Object Detection on PASCAL VOC 2007</H2>\n",
    "<H3 style=\"text-align: center\">Multi-Class Setting</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfPPQ6ztJhv4"
   },
   "source": [
    "For this Excercise, we will be finetuning a pre-trained [Faster R-CNN](https://arxiv.org/abs/1506.01497) model on the [PASCAL VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) dataset for Multiple Object Detection. It contains total 9,963 images containing 20 different objects: *aeroplane*, *bicycle*, *bird*, *boat*, *bottle*, *bus*, *car*, *cat*, *chair*, *cow*, *diningtable*, *dog*, *horse*, *motorbike*, *person*, *pottedplant*, *sheep*, *sofa*, *train*, *tvmonitor*. However, for this illustration we will use a subset of that dataset. First, we need to use the `pycocotools`, this library will be used for computing the evaluation metrics following the [Microsoft COCO](https://cocodataset.org/#home) metric for intersection over union. More detailed and complicated version of object detector can be found [here](https://github.com/facebookresearch/detectron2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuogFTV8o-vU"
   },
   "source": [
    "###Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j4rOhz-h1J3Z"
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import pycocotools\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es-ujgq7pDsZ"
   },
   "source": [
    "###Dataset\n",
    "We will only download the `trainval` set and split it in our way. In reality, PASCAL VOC has a separate `test` split which is generally used for comparing detection methods. Please have a look on the [PASCAL VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) dataset webpage for more details."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BaKsbseHe-im"
   },
   "source": [
    "if not os.path.exists('VOCtrainval_06-Nov-2007.tar'):\n",
    "    !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "if not os.path.exists('VOCtest_06-Nov-2007.tar'):\n",
    "    !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "!rm -r VOC2007/\n",
    "!mkdir VOC2007/\n",
    "!mkdir VOC2007/train/\n",
    "!mkdir VOC2007/test/\n",
    "!tar -xf VOCtrainval_06-Nov-2007.tar --directory VOC2007/train/\n",
    "!tar -xf VOCtest_06-Nov-2007.tar --directory VOC2007/test/\n",
    "!mv VOC2007/train/VOCdevkit/VOC2007/* VOC2007/train/\n",
    "!rm -r VOC2007/train/VOCdevkit/\n",
    "!mv VOC2007/test/VOCdevkit/VOC2007/* VOC2007/test/\n",
    "!rm -r VOC2007/test/VOCdevkit/\n",
    "class_str2num = {'aeroplane': 1, 'bicycle': 2, 'bird': 3, 'boat': 4, 'bottle': 5,\n",
    "                 'bus': 6, 'car': 7, 'cat': 8, 'chair': 9, 'cow': 10, \n",
    "                 'diningtable': 11, 'dog': 12, 'horse': 13, 'motorbike': 14, \n",
    "                 'person': 15, 'pottedplant': 16, 'sheep': 17, 'sofa': 18,\n",
    "                 'train': 19, 'tvmonitor': 20}\n",
    "class_num2str = {v: k for k, v in class_str2num.items()}\n",
    "antns = sorted(os.listdir('VOC2007/train/Annotations'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXivraQzp-6k"
   },
   "source": [
    "###Annotations\n",
    "Detection and segmentation datasets come with annotation files containing information about the object location (bounding box), pixel wise position etc. Below we can consider an example from PASCAL VOC annotation file.\n",
    "```\n",
    "<annotation>\n",
    "\t<folder>VOC2007</folder>\n",
    "\t<filename>000005.jpg</filename>\n",
    "\t<source>\n",
    "\t\t<database>The VOC2007 Database</database>\n",
    "\t\t<annotation>PASCAL VOC2007</annotation>\n",
    "\t\t<image>flickr</image>\n",
    "\t\t<flickrid>325991873</flickrid>\n",
    "\t</source>\n",
    "\t<owner>\n",
    "\t\t<flickrid>archintent louisville</flickrid>\n",
    "\t\t<name>?</name>\n",
    "\t</owner>\n",
    "\t<size>\n",
    "\t\t<width>500</width>\n",
    "\t\t<height>375</height>\n",
    "\t\t<depth>3</depth>\n",
    "\t</size>\n",
    "\t<segmented>0</segmented>\n",
    "\t<object>\n",
    "\t\t<name>chair</name>\n",
    "\t\t<pose>Rear</pose>\n",
    "\t\t<truncated>0</truncated>\n",
    "\t\t<difficult>0</difficult>\n",
    "\t\t<bndbox>\n",
    "\t\t\t<xmin>263</xmin>\n",
    "\t\t\t<ymin>211</ymin>\n",
    "\t\t\t<xmax>324</xmax>\n",
    "\t\t\t<ymax>339</ymax>\n",
    "\t\t</bndbox>\n",
    "\t</object>\n",
    "\t...\n",
    "\t<object>\n",
    "\t\t<name>chair</name>\n",
    "\t\t<pose>Unspecified</pose>\n",
    "\t\t<truncated>1</truncated>\n",
    "\t\t<difficult>1</difficult>\n",
    "\t\t<bndbox>\n",
    "\t\t\t<xmin>277</xmin>\n",
    "\t\t\t<ymin>186</ymin>\n",
    "\t\t\t<xmax>312</xmax>\n",
    "\t\t\t<ymax>220</ymax>\n",
    "\t\t</bndbox>\n",
    "\t</object>\n",
    "</annotation>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blZQOEwjpHtZ"
   },
   "source": [
    "###XML Parser\n",
    "We have written the following `XML` parser function for reading the annotation file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lmn929iA1N-N"
   },
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    for boxes in root.iter('object'):\n",
    "        filename = root.find('filename').text\n",
    "        ymin, xmin, ymax, xmax = None, None, None, None\n",
    "        ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
    "        box = [xmin, ymin, xmax, ymax]\n",
    "        bboxes.append(box)\n",
    "        labels.append(int(class_str2num[boxes.find(\"name\").text]))\n",
    "    return filename, bboxes, labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IvOAou-sH7h"
   },
   "source": [
    "###Example of Images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tX5PWJ8hnuTY"
   },
   "source": [
    "from PIL import Image, ImageDraw\n",
    "idx = np.random.randint(len(antns))\n",
    "filename, boxes, labels = parse_xml(os.path.join('VOC2007/train/Annotations', antns[idx]))\n",
    "image = Image.open(os.path.join('VOC2007/train/JPEGImages',filename))\n",
    "draw = ImageDraw.Draw(image)\n",
    "for i, ibox in enumerate(boxes):\n",
    "    draw.rectangle([(ibox[0], ibox[1]), (ibox[2], ibox[3])], outline='red', width=3)\n",
    "    draw.text((ibox[0], ibox[1]), text = class_num2str[labels[i]])\n",
    "image"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bHfLH5MvIwV"
   },
   "source": [
    "Clone the PyTorch vision repo and copy some python files containing implementation on evaluation, transformations, loss function etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VUzSp9Dqz-uj"
   },
   "source": [
    "!git clone https://github.com/pytorch/vision.git\n",
    "!cp vision/references/detection/utils.py ./\n",
    "!cp vision/references/detection/transforms.py ./\n",
    "!cp vision/references/detection/coco_eval.py ./\n",
    "!cp vision/references/detection/engine.py ./\n",
    "!cp vision/references/detection/coco_utils.py ./\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaD8xsZLswHk"
   },
   "source": [
    "###Data Loader\n",
    "An example of PyTorch data generator for object detector can be found [here](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/datasets.py) or [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). A simplified version of the data generator can be found below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "we2xyFam2Bo-"
   },
   "source": [
    "class PASCALVOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.antns = sorted(os.listdir(os.path.join(root, 'Annotations')))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load annotation\n",
    "        filename, boxes, labels = parse_xml(os.path.join(self.root, 'Annotations', self.antns[idx]))\n",
    "        # load image\n",
    "        img_path = os.path.join(self.root, 'JPEGImages', filename)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        num_objs = boxes.shape[0]\n",
    "        # classes\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = image_id\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.antns)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74gwm2Mrs7u-"
   },
   "source": [
    "###Data Instance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1nLLJhFSE82Q"
   },
   "source": [
    "dataset = PASCALVOCDataset(root='VOC2007/train')\n",
    "dataset.__getitem__(0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lekpWoWxtfUM"
   },
   "source": [
    "###Model\n",
    "In this workshop, we will be using [Faster R-CNN](https://arxiv.org/abs/1506.01497) which is a model that predicts both bounding boxes and class scores for potential objects in the image. The detection models available within PyTorch can be found [here](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection).\n",
    "\n",
    "![Faster R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image03.png)\n",
    "\n",
    "Check the PyTorch implementation of Faster R-CNN with ResNet-50 backbone [here](https://github.com/pytorch/vision/blob/master/torchvision/models/detection/faster_rcnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0voMlm9KFsD7"
   },
   "source": [
    "def get_model(num_classes):\n",
    "    # load an object detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygAsfSz8t5Gc"
   },
   "source": [
    "###Transformations\n",
    "We only use random horizontal flip transformation. One can use some other transformations, such as affine etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WXpj1L0lFyxM"
   },
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG8VuLMft-v3"
   },
   "source": [
    "###Initialization of Dataset and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0d9eZnYT3d4f"
   },
   "source": [
    "# use our dataset and defined transformations\n",
    "train_dataset = PASCALVOCDataset(root= 'VOC2007/train', transforms = get_transform(train=True))\n",
    "test_dataset = PASCALVOCDataset(root= 'VOC2007/test', transforms = get_transform(train=False))\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "train_indices = torch.randperm(len(train_dataset)).tolist()\n",
    "test_indices = torch.randperm(len(test_dataset)).tolist()\n",
    "# first n examples\n",
    "# because of the constraint of computational resources, I just use 100 samples.\n",
    "# Please feel free to use more samples if you have enough resources\n",
    "n = 100\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, train_indices[:n])\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, test_indices[:n])\n",
    "# define training and validation data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=utils.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=utils.collate_fn)\n",
    "print(\"We have: {} examples, {} are training and {} testing\".format(len(train_indices+test_indices), len(train_dataset), len(test_dataset)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mosmV1kIuRF5"
   },
   "source": [
    "###Initialization of Model and Optimizer\n",
    "In this training, it is recommended to use learning rate scheduler, more details on it can be found [here](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fj7oldkm4ErD"
   },
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# our dataset has two classes only - raccoon and not racoon\n",
    "num_classes = len(class_str2num) + 1\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7Z1Ut9vuWpN"
   },
   "source": [
    "###Train and Test Loop\n",
    "Read more about the `train_one_epoch()` and `evaluate()` functions [here](https://github.com/pytorch/vision/tree/master/references/detection). Details on the evaluation metrics, different parameters and following verbose can be found [here](https://cocodataset.org/#detection-eval). In this training, it is recommended to use learning rate scheduler and learning rate warm-up, more details on it can be found [here](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a_adRwyd4O-8"
   },
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, test_loader, device=device)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aC--SeVucpF"
   },
   "source": [
    "###Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aXwJxGTT4i6x"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "model = model.cpu()\n",
    "model.eval()\n",
    "for i in range(len(test_dataset)):\n",
    "    img, _ = test_dataset[i]\n",
    "    label_boxes = np.array(test_dataset[i][1]['boxes'])\n",
    "    #put the model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img])\n",
    "    image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    # draw groundtruth\n",
    "    for elem in range(len(label_boxes)):\n",
    "        draw.rectangle([(label_boxes[elem][0], label_boxes[elem][1]), (label_boxes[elem][2], label_boxes[elem][3])], outline =\"green\", width =3)\n",
    "    for element in range(len(prediction[0]['boxes'])):\n",
    "        box = prediction[0]['boxes'][element].cpu().numpy()\n",
    "        label = prediction[0]['labels'][element].cpu().item()\n",
    "        score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "        if score > 0.6:\n",
    "            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline ='red', width =3)\n",
    "            draw.text((box[0], box[1]), text = class_num2str[label] + ', ' + str(score))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}